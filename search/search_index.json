{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SomEnergia KPIs Documentation","text":""},{"location":"#per-lequip-tecnic","title":"Per l'Equip T\u00e8cnic","text":"<p>Comenceu aqu\u00ed</p>"},{"location":"#for-developers","title":"For Developers","text":"<p>The Readme or Readme.md is a good place to start. But if you're a dev making a new kpi, this will get you up to speed in 'I know kung-fu' style.</p>"},{"location":"#kpi-wars-a-new-kpi","title":"KPI wars: A new kpi","text":"<p>We have a script (<code>filtered_models_single_kpis.py</code>) that connects to the erp via erppeek and queries the kpis stated in a table called <code>erppeek_kpis_description</code>. So we first need to create that table.</p> <pre><code>$ python scripts/csv_to_sqltable.py --csvpath \"datasources/erppeek/erppeek_kpis_test.csv\" --dbapi \"postgresql://somenergia:PASSWORD@puppis.somenergia.lan:5432/sandbox\" --schema dbt_vader --table erppeek_kpis_description --ifexists replace\n</code></pre> <p>Now the kpis listed in <code>erppeek_kpis_test.csv</code> will be in a table under the schema <code>dbt_vader</code>. We can now run the <code>fitlered_models_single_kpis</code> script to compute them querying the ERP via erppeek.</p> <pre><code>$ python datasources/erppeek/filtered_models_single_kpis.py postgresql://somenergia:PASSWORD@puppis.somenergia.lan:5432/sandbox daily ERP_URL somenergia ERP_USER ERP_PASSWORD dbt_vader\n</code></pre> <p>Now we should have three tables with values, the EL is done and we are ready to Transform </p> <ul> <li>name: erppeek_kpis_description</li> <li>name: pilotatge_int_kpis</li> <li>name: pilotatge_float_kpis</li> </ul> <pre><code>$ dbt run --target testing --project-dir ./dbt_kpis -m kpis_raw+\n</code></pre> <p>You now have the <code>utils/show_query.sql</code> which you can set to whichever query you'd want dbt to output in the command-line if you don't want to go and check the tables yourself </p>"},{"location":"#the-new-hope","title":"The New Hope","text":"<p>However, if only raw data is needed \u2013as opposed to camps calculats\u2013, the current preferred approach is sync tables using airbyte and use dbt on top of them.</p>"},{"location":"#about-mkdocs","title":"About mkdocs","text":"<p>For full documentation on mkdocs visit mkdocs.org.</p> <p>In particular we are using mkdocs-material</p> <p>However, a quickstart is</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>poetry install --with docs</code></li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"Developers/2023-03-30-create_datadis_airbyte_connector/","title":"Create a datadis API airbyte connector","text":"<p>Clone the airbyte repository</p> <pre><code>git@github.com:airbytehq/airbyte.git\n</code></pre> <p>We will follow roughly The Exchange Rates connector tutorial and the cdk python info.</p> <p>The cookiecutter-ish connector dev template is started like so:</p> <pre><code>cd airbyte-integrations/connector-templates/generator\n./generate.sh\n</code></pre> <p>It will ask for a connector type (ptyhon http api in our case) and a name. <code>npm</code> must be available.</p> <p>airbyte connector acceptance tests require &gt;=python3.9</p>"},{"location":"Developers/2023-03-30-create_datadis_airbyte_connector/#run-the-source-using-python","title":"Run the source using python","text":"<p>As a script</p> <pre><code># from airbyte-integrations/connectors/source-&lt;name&gt;\npython main.py spec\npython main.py check --config secrets/config.json\npython main.py discover --config secrets/config.json\npython main.py read --config secrets/config.json --catalog sample_files/configured_catalog.json\n</code></pre>"},{"location":"Developers/2023-03-30-create_datadis_airbyte_connector/#run-the-source-using-docker","title":"Run the source using docker","text":"<p>As airbyte would do it</p> <pre><code># First build the container\ndocker build . -t airbyte/source-&lt;name&gt;:dev\n\n# Then use the following commands to run it\ndocker run --rm airbyte/source-&lt;name&gt;:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-&lt;name&gt;:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-&lt;name&gt;:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/sample_files:/sample_files airbyte/source-&lt;name&gt;:dev read --config /secrets/config.json --catalog /sample_files/configured_catalog.json\n</code></pre> <p>Then proceed to edit the <code>airbyte-integrations/connectors/source-datadis/source_datadis/spec.yaml</code> with the fields you'd want to show in the config. In our case, username and password.</p> <p>We also added the pypi datadis dependency in the setup.py of our connector.</p>"},{"location":"Developers/2023-03-30-create_datadis_airbyte_connector/#ok-back-from-the-cave","title":"OK back from the cave","text":"<p>run with <code>python main.py read --config secrets/config.json --catalog sample_files/configured_catalog.json</code></p> <p>First the <code>spec.yaml</code> is the configuration page, and what will be available with <code>config['']</code> in the source.py, nothing more, nothing less. It is filled with the <code>secrets/config.json</code>.</p> <p>What's more rellevant is the <code>configured_catalog.json</code> which defines how data records are delivered and the settings page of the source.</p> <p>You'll need a configured_catalog.json specifying the output schema. If you make it match with the api response we won't need to do any parsing.</p> <pre><code>{\n    \"streams\": [{\n        \"stream\": {\n            \"name\": \"get_consumption_data\",\n            \"source_defined_cursor\": false,\n            \"json_schema\": {\n                \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n                \"type\": \"object\",\n                \"properties\": {\n                    \"cups\": {\n                        \"type\": \"string\"\n                    },\n                    \"date\": {\n                        \"type\": \"string\",\n                        \"format\": \"date\"\n                    },\n                    \"time\": {\n                        \"type\": \"string\",\n                        \"airbyte_type\": \"time_without_timezone\"\n                    },\n                    \"consumptionKWh\": {\n                        \"type\": \"number\"\n                    },\n                    \"obtainMethod\": {\n                        \"type\": \"string\"\n                    }\n                }\n            },\n            \"supported_sync_modes\": [\"full_refresh\"]\n        },\n        \"sync_mode\": \"full_refresh\",\n        \"destination_sync_mode\": \"append\"\n    }]\n}\n</code></pre> <p>The params are set in the source settings, which is inconvenient because it means creating N sources per params settings. So, you'd be wondering, but startDate will change! The airbyte way of handling querying historical data is configuring an incremental read. That way the params would change accordingly.</p> <p>Also, we will need to query many CUPS. Maybe datadis allows an array as cups param, it does refer to them in plural. Another option might be asyncio-ing the requests but airbyte doesn't seem to be thought like that. One source/connection per CUPS at UI level seems a bad idea. Another option is programatically call/create the airbyte tasks, given a bunch of secrets.</p> <p>Airbyte facilitates pagination and other goodies, but datadis doesn't offer pagination on the non-agregated data :/</p> <p>source.py <pre><code>class DatadisStream(HttpStream, ABC):\n   # TODO: Fill in the url base. Required.\n    url_base = \"https://datadis.es/api-private/api/\"\n\n    max_retries = 0\n\n    # default strategy reads one record and then the rest, but datadis doesn't allow repeated requests with the same params :unamused:\n    availability_strategy = None\n\n    def __init__(self, config: Mapping[str, Any], **kwargs):\n        super().__init__()\n        self.username = config['username']\n        self.password = config['password']\n        self.cups = config['cups']\n        self.token = None\n\n    def path(\n        self, stream_state: Mapping[str, Any] = None, stream_slice: Mapping[str, Any] = None, next_page_token: Mapping[str, Any] = None\n    ) -&gt; str:\n        return \"get-consumption-data\"\n\n    def request_headers(\n        self, stream_state: Mapping[str, Any], stream_slice: Mapping[str, Any] = None, next_page_token: Mapping[str, Any] = None\n    ) -&gt; Mapping[str, Any]:\n        # The api requires that we include apikey as a header so we do that in this method\n        if not self.token:\n            username = self.username\n            password = self.password\n            self.token = asyncio.run(get_token(username, password))\n            print('refreshed token')\n\n        return {\"Authorization\": \"Bearer \"+self.token}\n\n    def request_params(\n        self, stream_state: Mapping[str, Any], stream_slice: Mapping[str, any] = None, next_page_token: Mapping[str, Any] = None\n    ) -&gt; MutableMapping[str, Any]:\n        \"\"\"\n        Override this method to define any query parameters to be set. Remove this method if you don't need to define request params.\n        Usually contains common params e.g. pagination size etc.\n\n        We hardcoded the startDate-endDate but it would have to be set in the secrets/config.json and incrementaled instead of full-refresh as this is.\n        \"\"\"\n        return {'cups': self.cups, 'distributorCode':2, 'measurementType':0, 'pointType':5, 'startDate': '2022/11', 'endDate': '2023/03', }\n\n    def parse_response(self, response: requests.Response, **kwargs) -&gt; Iterable[Mapping]:\n        \"\"\"\n        Override this method to define how a response is parsed.\n        :return an iterable containing each record in the response\n\n        if the json matches the catalog, you don't need any tranformation. If datadis supported pagination you could yield results I guess?\n        \"\"\"\n\n        return response.json()\n</code></pre></p> <p>If everything goes well you'll get something like this:</p> <pre><code>{\"type\": \"LOG\", \"log\": {\"level\": \"INFO\", \"message\": \"Starting syncing SourceDatadis\"}}\n{\"type\": \"LOG\", \"log\": {\"level\": \"INFO\", \"message\": \"Syncing stream: get_consumption_data \"}}\nrefreshed token\nRequested!\nRequest: /api-private/api/get-consumption-data?cups=XXX&amp;distributorCode=2&amp;measurementType=0&amp;pointType=5&amp;startDate=2022%2F11&amp;endDate=2023%2F03\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"get_consumption_data\", \"data\": {\"cups\": \"XXXX\", \"date\": \"2023/03/01\", \"time\": \"01:00\", \"consumptionKWh\": 0.076, \"obtainMethod\": \"Real\"}, \"emitted_at\": 1680281671795}}\n[...]\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"get_consumption_data\", \"data\": {\"cups\": \"XXX\", \"date\": \"2023/03/30\", \"time\": \"22:00\", \"consumptionKWh\": 0.0, \"obtainMethod\": \"Real\"}, \"emitted_at\": 1680281672153}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"get_consumption_data\", \"data\": {\"cups\": \"XXX\", \"date\": \"2023/03/30\", \"time\": \"23:00\", \"consumptionKWh\": 0.0, \"obtainMethod\": \"Real\"}, \"emitted_at\": 1680281672154}}\n{\"type\": \"RECORD\", \"record\": {\"stream\": \"get_consumption_data\", \"data\": {\"cups\": \"XXX\", \"date\": \"2023/03/30\", \"time\": \"24:00\", \"consumptionKWh\": 0.0, \"obtainMethod\": \"Real\"}, \"emitted_at\": 1680281672154}}\n{\"type\": \"LOG\", \"log\": {\"level\": \"INFO\", \"message\": \"Read 719 records from get_consumption_data stream\"}}\n{\"type\": \"LOG\", \"log\": {\"level\": \"INFO\", \"message\": \"Finished syncing get_consumption_data\"}}\n{\"type\": \"LOG\", \"log\": {\"level\": \"INFO\", \"message\": \"SourceDatadis runtimes:\\nSyncing stream get_consumption_data 0:00:05.670986\"}}\n{\"type\": \"LOG\", \"log\": {\"level\": \"INFO\", \"message\": \"Finished syncing SourceDatadis\"}}\n</code></pre> <p>The records are correctly read and parsed, then, theoretically we should implement another connector for the write or rather use a postgres connector and save this stream to a table with a few clicks.</p> <p>We would probably parse the date-time to datetime and drop the obtainMethod or apply more transforms.</p> <p>Then any other type of source could just implement their own source connector and then plug to our cedata-api destination connector, which we would have to implement, but then would be decoupled from any source imaginable.</p> <p>gotchas: datadis api doesn't like repeated requests so the availability and retries have to be disabled.</p> <p>Here we didn't test it using the docker call. Also, to get this to the official airbyte the whole PR- accept flow needs to be followed. On self-hosted, the soruces api url is configurable and could point to a personal github fork of airbyte and work.</p>"},{"location":"Developers/2023-05-12-timezones/","title":"Timezones segons dades","text":""},{"location":"Developers/2023-05-12-timezones/#context","title":"Context","text":"<p>Una mica de la festa de la timezone despr\u00e9s de barallar-nos-hi un xic.</p> <p>mantras: <code>Timestamp</code> is a picture of a clock. You don't want a picture of a clock.</p> <p>Sobre timestamptz, contrariament al qu\u00e8 hom pensaria quan llegeix \"timestamp with time zone\", aquest datatype de Postgres no guarda un timezone. \u00c9s un \"flag binari\" de visualizaci\u00f3, un helper d'inserci\u00f3 i de visualitzaci\u00f3 que converteix de la teva configuraci\u00f3 o la de servidor a unix timestamp.</p> <p>Sobre qu\u00e8 fem a dades, coincideix bastant amb el qu\u00e8 diu aqu\u00ed:</p> <p>Don't Do This</p>"},{"location":"Developers/2023-05-12-timezones/#glossari","title":"Glossari","text":"<p>Si posem <code>[citation needed]</code> vol dir que encara ho estem analitzant.</p> <p><code>show time zone</code> \u00e9s <code>Europe/Madrid</code>. Per tant les visualitzacions de timestamptz s'ensenyen en <code>Europe/Madrid</code></p>"},{"location":"Developers/2023-05-12-timezones/#resumet","title":"Resumet","text":"<p>Opcions:</p> <ol> <li>use timestamptz</li> <li>use timestamp (without time zone)</li> <li>use timestamp (without time zone) to store UTC times</li> </ol> <p>Triem la opci\u00f3 1. tot datetime a la db en timestamptz i quan insertes has de fer-ho amb el timezone especificat. En general no confiem amb <code>show time zone;</code> del servidor o client [citation needed perqu\u00e8 potser podr\u00edem acceptar agregacions na\u00eff si hi confi\u00e9ssim].</p> <p>Quan estem fent servir <code>timescaledb</code> fem agregacions amb la funci\u00f3 <code>time_bucket</code> per que el query planner s\u00e0piga com accedir els chunks correctament. En altre cas, fem servir agregacions amb <code>date_trunc('day', some_timestamptz, 'Europe/Madrid')</code></p> <p>Les dates poden ser na\u00eff (no existeix el concepte de date aware a postgres), per\u00f2 seran en local. Si es pot i t\u00e9 sentit, mantenir el timestamptz de mitjanit [citation needed].</p>"},{"location":"Developers/2023-05-12-timezones/#aprofundim","title":"Aprofundim","text":""},{"location":"Developers/2023-05-12-timezones/#types-dentrada","title":"types d'entrada","text":"<p>si rebem un timestamp, de seguida el passem a timestamptz amb <code>at time zone</code>.</p> <p>si rebem un date, primer el passem a timestamp</p> <p>Amb el timezone del client configurat a 'Europe/Madrid', d\u00f3na</p> <pre><code>select\n    '2021-01-01'::date as adate,\n    '2021-01-01'::date at time zone 'Europe/Madrid', -- timestamp \ud83d\ude32 \u274c\n    '2021-01-01'::date::timestamp at time zone 'Europe/Madrid' -- timestamptz \ud83d\udc4d\n</code></pre> adate timezone timezone 2021-01-01 2021-01-01 00:00:00 2021-01-01 00:00:00+01"},{"location":"Developers/2023-05-12-timezones/#sources-amb-naif-i-columna-dst","title":"Sources amb na\u00eff i columna dst","text":"<p>Ho passem a timestamp i convertim a la timezone que correspongui abans d'insertar-ho en timestamptz</p> <pre><code>    CASE\n        WHEN estiu=1 AND (sistema = 'PEN' or sistema = 'BAL')\n            THEN data::timestamp AT TIME ZONE 'CEST'\n        WHEN estiu=0 AND (sistema = 'PEN' or sistema = 'BAL')\n            THEN data::timestamp AT TIME ZONE 'CET'\n        WHEN estiu=1 AND sistema = 'CAN'\n            THEN data::timestamp AT TIME ZONE 'WETDST'\n        WHEN estiu=0 AND sistema = 'CAN'\n            THEN data::timestamp AT TIME ZONE 'WET'\n    END AS end_hour_aware\n</code></pre> <p>Idealment afegir\u00edem una columna amb el timezone en sintaxi postgres ('Europe/Madrid', 'Atlantic/Canary') per despr\u00e9s poder fer</p> <pre><code>select\nend_hour_aware,\nend_hour_aware at time zone timezone as end_hour_local,\ndate_trunc('day', end_hour_aware, timezone) as day_local\nfrom (\n    values\n        ('2021-01-01 10:00:00'::timestamp at time zone 'Europe/Madrid', 'Europe/Madrid'),\n        ('2021-01-01 10:00:00'::timestamp at time zone 'Atlantic/Canary', 'Atlantic/Canary')\n) as foo(end_hour_aware, timezone);\n</code></pre> end_hour_aware end_hour_local day_local 2021-01-01 10:00:00+01 2021-01-01 10:00:00 2021-01-01 00:00:00+01 2021-01-01 11:00:00+01 2021-01-01 10:00:00 2021-01-01 01:00:00+01 <p>Podeu veure la casu\u00edstica:</p> <pre><code>select\nend_hour_aware at time zone 'UTC' as end_hour_naif_utc_in_db, -- db *always* stores na\u00eff unix timestamps, utc, even if datatype is timestamptz\nend_hour_aware as end_hour_aware_at_configured_timezone, -- depends on show time zone; of your client/server\nend_hour_aware at time zone timezone as end_hour_local, -- timestamp na\u00eff (can't be otherwise once we localize)\ndate_trunc('day', end_hour_aware, timezone) as midnight_local, -- midnight local seen by `show time zone;`, it's timestamptz, hence automatically converted for display. really unix_timestamp in db\ndate_trunc('day', end_hour_aware, timezone)::date as day_local,\ndate_trunc('day', end_hour_aware)::date as day_naif_local_and_wrong, -- \u274c implicit conversion to `show time zone`\ntime_bucket('1 day', end_hour_aware, timezone) as day_bucket\nfrom (\n    values\n        ('2021-01-01 20:00:00'::timestamp at time zone 'Europe/Madrid', 'Europe/Madrid'),\n        ('2021-01-01 20:00:00'::timestamp at time zone 'Atlantic/Canary', 'Atlantic/Canary'),\n        ('2021-01-01 20:00:00'::timestamp at time zone 'PST', 'PST')\n) as foo(end_hour_aware, timezone);\n</code></pre> end_hour_naif_utc_in_db end_hour_aware_at_configured_timezone end_hour_local midnight_local day_local day_naif_local_and_wrong day_bucket 2021-01-01 19:00:00 2021-01-01 20:00:00+01 2021-01-01 20:00:00 2021-01-01 00:00:00+01 2021-01-01 2021-01-01 2021-01-01 00:00:00+01 2021-01-01 20:00:00 2021-01-01 21:00:00+01 2021-01-01 20:00:00 2021-01-01 01:00:00+01 2021-01-01 2021-01-01 2021-01-01 01:00:00+01 2021-01-02 04:00:00 2021-01-02 05:00:00+01 2021-01-01 20:00:00 2021-01-01 09:00:00+01 2021-01-01 2021-01-02 2021-01-01 09:00:00+01"},{"location":"Developers/2023-05-12-timezones/#agregacions","title":"Agregacions","text":"<p>Les agregacions (de calendari) sempre amb el time zone que sigui rellevant. Una agregaci\u00f3 diaria est\u00e0 sempre lligada a un timezone concret, perqu\u00e8 el dia est\u00e0 definit nom\u00e9s dins d'un timezone, sin\u00f3 parlar\u00edem d'agregacions de 24h, que far\u00edem en utc.</p> <pre><code>select\n    -- date_trunc per defecte fa servir el time zone configurat\n    date_trunc('day', '2021-01-01 01:00:00+05:00'::timestamptz) as date_trunc_local,\n    date_trunc('day', '2021-01-01 01:00:00+05:00'::timestamptz, 'Europe/Madrid') as date_trunc_local_explicit,\n    -- time_bucket per defecte fa servir utc\n    time_bucket('1 day', '2021-01-01 01:00:00+05:00'::timestamptz) as time_bucket_utc,\n    time_bucket('1 day', '2021-01-01 01:00:00+05:00'::timestamptz, 'Europe/Madrid') as time_bucket_local;\n</code></pre> date_trunc_local date_trunc_local_explicit time_bucket_utc time_bucket_local 2020-12-31 00:00:00+01 2020-12-31 00:00:00+01 2020-12-31 01:00:00+01 2020-12-31 00:00:00+01 <p>En general ho far\u00edem tot en el time zone de l'Estat, per\u00f2 dep\u00e8n del use case (Veure Excepcions a la norma).</p> <p>Si hem de convertir a date caldr\u00e0 passar-ho al time zone que toqui abans de convertir a date, com que no tenim una gunci\u00f3 time_bucket(timestamptz)-&gt;date (ni date_trunc) cal que abans de convertir a date ho passem a na\u00eff del timezone que toqui per a qu\u00e8 postgres no li apliqui el time zone que tinguem configurat.</p> <pre><code>select '2022-12-31 23:00:00+00'::timestamptz,\n'2022-12-31 23:00:00+00'::timestamptz at time zone 'Europe/Madrid',\n'2022-12-31 23:00:00'::timestamp at time zone 'Europe/Madrid' as naif,\ndate_trunc('day', '2022-12-31 23:00:00+00'::timestamptz, 'Europe/Madrid'),\ndate_trunc('day', '2022-12-31 23:00:00+00'::timestamptz, 'Europe/Madrid')::date as incorrect_dt_based_on_config,\n(date_trunc('day', '2022-12-31 23:00:00+00'::timestamptz, 'Europe/Madrid') at time zone 'Europe/Madrid')::date as correct_dt,\n(time_bucket('1 day', '2022-12-31 23:00:00+00'::timestamptz, 'Europe/Madrid') at time zone 'Europe/Madrid'),\n(time_bucket('1 day', '2022-12-31 23:00:00+00'::timestamptz, 'Europe/Madrid'))::date as incorrect_tb_based_on_config,\n(time_bucket('1 day', '2022-12-31 23:00:00+00'::timestamptz, 'Europe/Madrid') at time zone 'Europe/Madrid')::date as correct\n</code></pre> timestamptz timezone naif date_trunc incorrect_dt_based_on_config correct_dt timezone incorrect_tb_based_on_config correct 2022-12-31 23:00:00+00 2023-01-01 00:00:00 2022-12-31 22:00:00+00 2022-12-31 23:00:00+00 2022-12-31 2023-01-01 2023-01-01 00:00:00 2022-12-31 2023-01-01"},{"location":"Developers/2023-05-12-timezones/#excepcions-a-la-norma","title":"Excepcions a la norma","text":"<p>Pel cas que estem tractant actualment, previsi\u00f3 de la demanda, fem servir el dia local (a picture of a clock) perqu\u00e8 el qu\u00e8 ens interessa no \u00e9s comparar intervals de temps, sin\u00f3 comportaments del dilluns, del cap de setmana, etc. Les hores a agrupar, els dies a agrupar, s\u00f3n culturals, encara que de fet representin packets d'hores universals diferents [citation needed].</p>"},{"location":"Developers/2023-05-12-timezones/#problemes","title":"problemes","text":"<p>En general aquest recull Don't do this est\u00e0 for\u00e7a b\u00e9.</p>"},{"location":"Developers/2023-05-12-timezones/#timestamptz-a-tot-arreu","title":"timestamptz a tot arreu","text":"<p><code>timestamptz</code> no est\u00e0 suportat a tot arreu, hi ha ORMs que no ho suporten b\u00e9.</p> <p>Tothom ha de ser conscient que ha d'inserir explicitant el timezone o b\u00e9 assegurant-se que el timezone configurat del servidor \u00e9s el qu\u00e8 ell assumeix que \u00e9s. Com que aix\u00f2 \u00faltim \u00e9s un pitfall, millor sempre passar a timestamptz de seguida i explicitament.</p>"},{"location":"Developers/2023-05-12-timezones/#timestamp-naif-utc","title":"timestamp na\u00eff utc","text":"<p>Aix\u00f2 implica que tothom s\u00e0piga que els timestamps s\u00f3n na\u00effs sem\u00e0nticament. TODO: Elaborar maneres alternatives de fer inserts a la proposada al Don't do this</p>"},{"location":"Developers/2023-05-12-timezones/#timestamp-naif-europemadrid","title":"timestamp na\u00eff 'Europe/Madrid'","text":"<p>Si no tindr\u00e0s mai de la vida altres timezones... per\u00f2 no ho recomanem, perqu\u00e8 despr\u00e9s passa el qu\u00e8 passa</p>"},{"location":"Developers/dbt_cheatsheet/","title":"Coses \u00fatils de DBT","text":""},{"location":"Developers/dbt_cheatsheet/#how-to-install-dbt-utils","title":"How to install dbt utils","text":"<p>Per poder incorporar funcions i macros ampliades com pivot() cal instal\u00b7lar DBT utils.</p> <pre><code>dbt deps --project-dir dbt_kpis\n</code></pre>"},{"location":"Developers/kpi_dev_workflow/","title":"Kpi dev workflow","text":""},{"location":"Developers/kpi_dev_workflow/#kpis-dev-workflow-extended","title":"kpis dev workflow extended","text":"<p>More details on variations of the main workflow to come</p>"},{"location":"Developers/production_deploy/","title":"How to deploy to production environment","text":"<p>tl; dr aka muxo testo</p> <pre><code>git merge YOUR_BRANCH\n\npython scripts/csv_to_sqltable.py --csvpath \"datasources/erppeek/erppeek_kpis_decription.csv\" --dbapi \"postgresql://somenergia:PASSWORD@puppis.somenergia.lan:5432/dades\" --schema prod_operational --table erppeek_kpis_description --ifexists append --truncate\n\ngit push\n\n#Run Airflow DAG\n\ndbt run --target prod -m +kpis_row+\n</code></pre>"},{"location":"Developers/production_deploy/#merge-your-branch-into-main","title":"Merge your branch into main","text":"<p> Airflow is in Continuous Delivery, <code>main</code> branch will be automatically downloaded in production as soon as a task is run. </p> <pre><code>git pull\ngit merge main\ngit checkout main\ngit merge YOUR_BRANCH\n</code></pre> <p>When you're ready, push to production</p> <pre><code>git push\n</code></pre>"},{"location":"Developers/production_deploy/#update-kpis-description-table","title":"Update KPIs Description table","text":"<p>From your local machine launch script to update KPIs table from CSV.</p> <p>This process overwrites the table.</p> <p>Given that dbt views depend on the table, we can't drop it. Therefore we truncate and append.</p> <pre><code>python scripts/csv_to_sqltable.py --csvpath \"datasources/erppeek/erppeek_kpis_description.csv\" --dbapi \"postgresql://somenergia:PASSWORD@puppis.somenergia.lan:5432/dades\" --schema prod_operational --table erppeek_kpis_description --ifexists append --truncate\n</code></pre>"},{"location":"Developers/production_deploy/#launch-airflow-dag-or-wait-for-it","title":"Launch Airflow DAG or wait for it","text":"<p>Airflow runs daily tasks reading the kpis table and querying the ERP via erppeek. You can wait for it to run or run it manually. DBT already selects the newest run when publishing the kpis to the datamart.</p>"},{"location":"Developers/production_deploy/#run-dbt-workflow","title":"Run DBT workflow","text":"<p>From your local machine run DBT workflow targeting production environment.</p> <pre><code>dbt run --target prod -m +kpis_wide+\n</code></pre> <p>or</p> <pre><code>dbt run --target prod\n</code></pre>"},{"location":"Developers/adr/2022-09-02-dbt_or_not_dbt/","title":"Rationale darrera de l'\u00fas de DBT","text":"<ul> <li>Status:accepted</li> <li>Deciders: Roger, Luc\u00eda, Pol</li> <li>Date: 2022-09-02</li> </ul>"},{"location":"Developers/adr/2022-09-02-dbt_or_not_dbt/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Actualment tenim processos de transformaci\u00f3 duplicats, sense control de versions ni cap manteniment escampats entre queries del Superset i Redash.</p> <ul> <li>materialized views incremental per grans volumns de dades</li> <li>quan canvio una view costa saber quines views haig de canviar i el deploy \u00e9s dif\u00edcil</li> <li>\u00c9s pesat duplicar processos manualent que s\u00f3n pr\u00e0cticament iguals (agg diari, setmanal, p.e.)</li> </ul>"},{"location":"Developers/adr/2022-09-02-dbt_or_not_dbt/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>Materializar incrementalment</li> <li>control sobre modificar una taula/view intermitja</li> <li>deploy r\u00e0pid</li> <li>SQL Gitejat</li> <li>Autom\u00e0ticament diferents opcions d'una query semblant (agg setmanal, diari, etc)</li> </ul>"},{"location":"Developers/adr/2022-09-02-dbt_or_not_dbt/#tensionsfriccions-a-explorar","title":"Tensions/friccions a explorar","text":"<ul> <li>Por a overdesign: moltes carpetes, molts fitxers, moltes views.. total per fer una suma</li> <li>Chaos de nomenclatures de fitxers</li> <li>no tenir \"funcions\" reutilitzables --&gt; macros?</li> <li>parametritzacions</li> <li>Unit testing o modelatge</li> <li>timescale + dbt ? Ning\u00fa ho ha fet, perqu\u00e8?</li> </ul>"},{"location":"Developers/adr/2022-09-02-dbt_or_not_dbt/#considered-options","title":"Considered Options","text":"<ul> <li>continuous agregates de timescale</li> <li>custom scripts (publish.sh ...)</li> <li>dbt</li> <li>python scripting controlat via Airflow</li> </ul>"},{"location":"Developers/adr/2022-09-02-dbt_or_not_dbt/#decision-outcome","title":"Decision Outcome","text":"<p>timescale</p> <p>countinuous agregats de timescale t\u00e9 moltes limitacions</p> <p>dbt</p> <p>DBT va agafant avantatge.</p>"},{"location":"Developers/adr/2022-09-02-dbt_or_not_dbt/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Single Truth Simplificat</li> <li>No c\u00f3rrer en RAM</li> <li>dbt afegeix algunes columnes quality of life : create_date</li> </ul>"},{"location":"Developers/adr/2022-09-02-dbt_or_not_dbt/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>No Pandas :( (fal potser serveix com alternativa)</li> <li>Fer transformacions complexes en SQL \u00e9s perjudicial per la salut</li> </ul>"},{"location":"Developers/adr/2022-09-02-dbt_or_not_dbt/#links","title":"Links","text":"<ul> <li>dbt</li> <li>timescale</li> </ul>"},{"location":"Developers/adr/2022-09-07-top_down_arch_detail/","title":"Rationale darrera de l'arquitectura de dades","text":"<ul> <li>Status: en elaboraci\u00f3</li> <li>Deciders: Roger, Luc\u00eda, Pol</li> <li>Date: 2022-09-07</li> </ul>"},{"location":"Developers/adr/2022-09-07-top_down_arch_detail/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Necessitem establir un est\u00e0ndar de gesti\u00f3 de dades a SomEnergia</p> <p>schema_data ware</p>"},{"location":"Developers/adr/2022-09-07-top_down_arch_detail/#decision-drivers","title":"Decision Drivers","text":""},{"location":"Developers/adr/2022-09-07-top_down_arch_detail/#tensionsfriccions-a-explorar","title":"Tensions/friccions a explorar","text":""},{"location":"Developers/adr/2022-09-07-top_down_arch_detail/#considered-options","title":"Considered Options","text":""},{"location":"Developers/adr/2022-09-07-top_down_arch_detail/#decision-outcome","title":"Decision Outcome","text":""},{"location":"Developers/adr/2022-09-07-top_down_arch_detail/#positive-consequences","title":"Positive Consequences","text":""},{"location":"Developers/adr/2022-09-07-top_down_arch_detail/#negative-consequences","title":"Negative Consequences","text":""},{"location":"Developers/adr/2022-09-07-top_down_arch_detail/#links","title":"Links","text":""},{"location":"Developers/adr/2022-10-06-bundle_of_aggregates_in_marts/","title":"Time-series agregation code duplication prevention","text":"<ul> <li>Status: proposed</li> <li>Deciders: Equip de Dades</li> <li>Date: 2022-10-06</li> </ul> <p>Technical Story:</p>"},{"location":"Developers/adr/2022-10-06-bundle_of_aggregates_in_marts/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Un dels motius de dbt \u00e9s reduir code duplication per les agregacions (diaria, setmanal, mensual, trimestral, anual...), com ho podem fer a dbt?</p> <p>As-is necessitem un model per cada agregaci\u00f3 per cada taula final, que no ens evita la duplicaci\u00f3 de codi tot perqu\u00e8 cada agregaci\u00f3 necessita el seu model</p>"},{"location":"Developers/adr/2022-10-06-bundle_of_aggregates_in_marts/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>[driver 1, e.g., a force, facing concern, \u2026]</li> <li>[driver 2, e.g., a force, facing concern, \u2026]</li> <li>\u2026 </li> </ul>"},{"location":"Developers/adr/2022-10-06-bundle_of_aggregates_in_marts/#considered-options","title":"Considered Options","text":"<ol> <li>soft-linked macro on every data mart</li> <li>just repeat models with macros for aggregations</li> <li>use post-hooks to move tables 1</li> <li>other stuff</li> <li>dbt metrics support!</li> </ol>"},{"location":"Developers/adr/2022-10-06-bundle_of_aggregates_in_marts/#decision-outcome","title":"Decision Outcome","text":"<ol> <li>dbt metrics support (en discussi\u00f3)</li> </ol>"},{"location":"Developers/adr/2022-10-06-bundle_of_aggregates_in_marts/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>[e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]</li> <li>\u2026</li> </ul>"},{"location":"Developers/adr/2022-10-06-bundle_of_aggregates_in_marts/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>[e.g., compromising quality attribute, follow-up decisions required, \u2026]</li> <li>\u2026</li> </ul>"},{"location":"Developers/adr/2022-10-06-bundle_of_aggregates_in_marts/#pros-and-cons-of-the-options","title":"Pros and Cons of the Options","text":""},{"location":"Developers/adr/2022-10-06-bundle_of_aggregates_in_marts/#option-1","title":"[option 1]","text":"<p>[example | description | pointer to more information | \u2026] </p> <ul> <li>Good, because [argument a]</li> <li>Good, because [argument b]</li> <li>Bad, because [argument c]</li> <li>\u2026 </li> </ul>"},{"location":"Developers/adr/2022-10-06-bundle_of_aggregates_in_marts/#option-2","title":"[option 2]","text":"<p>[example | description | pointer to more information | \u2026] </p> <ul> <li>Good, because [argument a]</li> <li>Good, because [argument b]</li> <li>Bad, because [argument c]</li> <li>\u2026 </li> </ul>"},{"location":"Developers/adr/2022-10-06-bundle_of_aggregates_in_marts/#option-3","title":"[option 3]","text":"<p>[example | description | pointer to more information | \u2026] </p> <ul> <li>Good, because [argument a]</li> <li>Good, because [argument b]</li> <li>Bad, because [argument c]</li> <li>\u2026 </li> </ul>"},{"location":"Developers/adr/2022-10-06-bundle_of_aggregates_in_marts/#links","title":"Links","text":"<ul> <li>[Link type] [Link to ADR] </li> <li>\u2026 </li> </ul>"},{"location":"Developers/adr/2022-10-06-event_driven_data/","title":"Event driven data","text":"<ul> <li>Status: en discussi\u00f3</li> <li>Deciders: Equip de Dades</li> <li>Date: 2022-10-06</li> </ul> <p>Technical Story:</p> <p>En Juanpe m'ha comentat Apache Kafka. Ara mateix estem enfocats a batch-processing data pipelines. Volem suportar tamb\u00e9 event-driven data pipelines? Si si, com i perqu\u00e8?</p>"},{"location":"Developers/adr/2022-10-06-event_driven_data/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]</p>"},{"location":"Developers/adr/2022-10-06-event_driven_data/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>[driver 1, e.g., a force, facing concern, \u2026]</li> <li>[driver 2, e.g., a force, facing concern, \u2026]</li> <li>\u2026 </li> </ul>"},{"location":"Developers/adr/2022-10-06-event_driven_data/#considered-options","title":"Considered Options","text":"<ul> <li>[option 1]</li> <li>[option 2]</li> <li>[option 3]</li> <li>\u2026 </li> </ul>"},{"location":"Developers/adr/2022-10-06-event_driven_data/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)].</p>"},{"location":"Developers/adr/2022-10-06-event_driven_data/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>[e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]</li> <li>\u2026</li> </ul>"},{"location":"Developers/adr/2022-10-06-event_driven_data/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>[e.g., compromising quality attribute, follow-up decisions required, \u2026]</li> <li>\u2026</li> </ul>"},{"location":"Developers/adr/2022-10-06-event_driven_data/#pros-and-cons-of-the-options","title":"Pros and Cons of the Options","text":""},{"location":"Developers/adr/2022-10-06-event_driven_data/#option-1","title":"[option 1]","text":"<p>[example | description | pointer to more information | \u2026] </p> <ul> <li>Good, because [argument a]</li> <li>Good, because [argument b]</li> <li>Bad, because [argument c]</li> <li>\u2026 </li> </ul>"},{"location":"Developers/adr/2022-10-06-event_driven_data/#option-2","title":"[option 2]","text":"<p>[example | description | pointer to more information | \u2026] </p> <ul> <li>Good, because [argument a]</li> <li>Good, because [argument b]</li> <li>Bad, because [argument c]</li> <li>\u2026 </li> </ul>"},{"location":"Developers/adr/2022-10-06-event_driven_data/#option-3","title":"[option 3]","text":"<p>[example | description | pointer to more information | \u2026] </p> <ul> <li>Good, because [argument a]</li> <li>Good, because [argument b]</li> <li>Bad, because [argument c]</li> <li>\u2026 </li> </ul>"},{"location":"Developers/adr/2022-10-06-event_driven_data/#links","title":"Links","text":"<ul> <li>[Link type] [Link to ADR] </li> <li>\u2026 </li> </ul>"},{"location":"Developers/adr/2022-10-06-olap_cubes/","title":"OLAP cubes","text":"<ul> <li>Status: accepted</li> <li>Deciders: Equip de Dades</li> <li>Date: 2022-10-06</li> </ul> <p>Technical Story:</p> <p>OLAP cubes?</p>"},{"location":"Developers/adr/2022-10-06-olap_cubes/#context-and-problem-statement","title":"Context and Problem Statement","text":""},{"location":"Developers/adr/2022-10-06-olap_cubes/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>[driver 1, e.g., a force, facing concern, \u2026]</li> <li>[driver 2, e.g., a force, facing concern, \u2026]</li> <li>\u2026 </li> </ul>"},{"location":"Developers/adr/2022-10-06-olap_cubes/#considered-options","title":"Considered Options","text":"<ol> <li>OLAP i algun vendor o nos\u00e9</li> <li>datasets basats en dbt models via macros on-demand</li> </ol>"},{"location":"Developers/adr/2022-10-06-olap_cubes/#decision-outcome","title":"Decision Outcome","text":"<p>2. datasets basats en dbt models</p> <p>A revisar en el futur</p>"},{"location":"Developers/adr/2022-10-06-olap_cubes/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>[e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]</li> <li>\u2026</li> </ul>"},{"location":"Developers/adr/2022-10-06-olap_cubes/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>[e.g., compromising quality attribute, follow-up decisions required, \u2026]</li> <li>\u2026</li> </ul>"},{"location":"Developers/adr/2022-10-06-olap_cubes/#pros-and-cons-of-the-options","title":"Pros and Cons of the Options","text":""},{"location":"Developers/adr/2022-10-06-olap_cubes/#olap-cube","title":"OLAP cube","text":"<p>[example | description | pointer to more information | \u2026] </p> <ul> <li>Good, because [argument a]</li> <li>Good, because [argument b]</li> <li>Bad, because [argument c]</li> <li>\u2026 </li> </ul>"},{"location":"Developers/adr/2022-10-06-olap_cubes/#datasets","title":"datasets","text":"<p>Molta gent a dbt argumenta en contra de OLAP cubes i a favor de datasets</p> <ul> <li>modern datawarehouses no tenen limitacions d'espai, la limitaci\u00f3 \u00e9s el dev-time</li> <li>Necessita pre-calcular totes les combinacions de dimensions en la granularitat m\u00ednima 1</li> <li>Molt cost\u00f3s de dissenyar (requereix stars de kimball) 2</li> </ul>"},{"location":"Developers/adr/2022-10-06-olap_cubes/#links","title":"Links","text":"<ul> <li>[Link type] [Link to ADR] </li> <li>\u2026 </li> </ul>"},{"location":"Developers/adr/2022-10-06-pivoting/","title":"Pivoting","text":"<ul> <li>Status: accepted</li> <li>Deciders: Equip de Dades</li> <li>Date: 2022-10-06</li> </ul> <p>Technical Story:</p>"},{"location":"Developers/adr/2022-10-06-pivoting/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Per fer els indicadors derivats ens conv\u00e9 tenir dades amples on cada columna \u00e9s l'indicador i la primera fila create date</p>"},{"location":"Developers/adr/2022-10-06-pivoting/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>[driver 1, e.g., a force, facing concern, \u2026]</li> <li>[driver 2, e.g., a force, facing concern, \u2026]</li> <li>\u2026 </li> </ul>"},{"location":"Developers/adr/2022-10-06-pivoting/#considered-options","title":"Considered Options","text":"<ol> <li>dbt pivot (modified)</li> <li>crosstab de postgres</li> <li>pandas (via fal)</li> </ol>"},{"location":"Developers/adr/2022-10-06-pivoting/#decision-outcome","title":"Decision Outcome","text":"<p>Hem triat 1. dbt pivot perqu\u00e8 resol el tema en pur sql i views.</p>"},{"location":"Developers/adr/2022-10-06-pivoting/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>[e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]</li> <li>\u2026</li> </ul>"},{"location":"Developers/adr/2022-10-06-pivoting/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>[e.g., compromising quality attribute, follow-up decisions required, \u2026]</li> <li>\u2026</li> </ul>"},{"location":"Developers/adr/2022-10-06-pivoting/#pros-and-cons-of-the-options","title":"Pros and Cons of the Options","text":""},{"location":"Developers/adr/2022-10-06-pivoting/#dbt-pivot","title":"dbt pivot","text":"<p>[example | description | pointer to more information | \u2026] </p> <ul> <li>Good, because [argument a]</li> <li>Good, because [argument b]</li> <li>Bad, because [argument c]</li> <li>\u2026 </li> </ul>"},{"location":"Developers/adr/2022-10-06-pivoting/#crosstab","title":"crosstab","text":"<p>[example | description | pointer to more information | \u2026] </p> <ul> <li>Good, because [argument a]</li> <li>Good, because [argument b]</li> <li>Bad, because [argument c]</li> <li>\u2026 </li> </ul>"},{"location":"Developers/adr/2022-10-06-pivoting/#pandas-fal","title":"pandas fal","text":"<ul> <li>Good, because very flexible</li> <li>Bad, because </li> <li>\u2026 </li> </ul>"},{"location":"Developers/adr/2022-10-06-pivoting/#links","title":"Links","text":"<ul> <li>[Link type] [Link to ADR] </li> <li>\u2026 </li> </ul>"},{"location":"Developers/adr/template/","title":"Template","text":""},{"location":"Developers/adr/template/#short-title-of-solved-problem-and-solution","title":"[short title of solved problem and solution]","text":"<ul> <li>Status: [proposed | rejected | accepted | deprecated | \u2026 | superseded by <code>[ADR-0005](0005-example.md)]</code> </li> <li>Deciders: [list everyone involved in the decision] </li> <li>Date: [YYYY-MM-DD when the decision was last updated] </li> </ul> <p>Technical Story: [description | ticket/issue URL] </p>"},{"location":"Developers/adr/template/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]</p>"},{"location":"Developers/adr/template/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>[driver 1, e.g., a force, facing concern, \u2026]</li> <li>[driver 2, e.g., a force, facing concern, \u2026]</li> <li>\u2026 </li> </ul>"},{"location":"Developers/adr/template/#considered-options","title":"Considered Options","text":"<ul> <li>[option 1]</li> <li>[option 2]</li> <li>[option 3]</li> <li>\u2026 </li> </ul>"},{"location":"Developers/adr/template/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)].</p>"},{"location":"Developers/adr/template/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>[e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]</li> <li>\u2026</li> </ul>"},{"location":"Developers/adr/template/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>[e.g., compromising quality attribute, follow-up decisions required, \u2026]</li> <li>\u2026</li> </ul>"},{"location":"Developers/adr/template/#pros-and-cons-of-the-options","title":"Pros and Cons of the Options","text":""},{"location":"Developers/adr/template/#option-1","title":"[option 1]","text":"<p>[example | description | pointer to more information | \u2026] </p> <ul> <li>Good, because [argument a]</li> <li>Good, because [argument b]</li> <li>Bad, because [argument c]</li> <li>\u2026 </li> </ul>"},{"location":"Developers/adr/template/#option-2","title":"[option 2]","text":"<p>[example | description | pointer to more information | \u2026] </p> <ul> <li>Good, because [argument a]</li> <li>Good, because [argument b]</li> <li>Bad, because [argument c]</li> <li>\u2026 </li> </ul>"},{"location":"Developers/adr/template/#option-3","title":"[option 3]","text":"<p>[example | description | pointer to more information | \u2026] </p> <ul> <li>Good, because [argument a]</li> <li>Good, because [argument b]</li> <li>Bad, because [argument c]</li> <li>\u2026 </li> </ul>"},{"location":"Developers/adr/template/#links","title":"Links","text":"<ul> <li>[Link type] [Link to ADR] </li> <li>\u2026 </li> </ul>"},{"location":"ET/2023-07-31-Overview/","title":"Overview dels KPIs","text":"<p>Teniu els kpis al superset.</p> <p>Cada equip tindr\u00e0 el seu dashboard. Podeu veure'n la llista completa aqu\u00ed</p> <p>No dubteu en fer-nos arribar incid\u00e8ncies o coses estranyes que veigueu via el formulari d'incid\u00e8ncies!</p>"},{"location":"models_meta/2023-03-22-dades_auto_generalitat/","title":"Demanda d'an\u00e0lisi d'autoconsums encallats per la Generalitat","text":"<p>Per una trobada amb la Generalitat sobre els tr\u00e0mits d'autoconsum, l'Herm\u00ednia ens va demanar un an\u00e0lisi de l'estat dels tr\u00e0mits de contractes amb auto compartida.</p> <p>Context de helpscout</p> <p>Contractes amb auto subjectes d'an\u00e0lisi:</p> <ul> <li>[42] - Con excedentes y compensaci\u00f3n Colectivo\u2013 Consumo</li> <li>[43] - Con excedentes y compensaci\u00f3n Colectivo a trav\u00e9s de red\u2013 Consumo</li> </ul> <p>El codi sql analyse de dbt es troba a dbt_kpis</p>"},{"location":"models_meta/2023-03-22-dades_auto_generalitat/#necessitat-1","title":"Necessitat 1","text":"<ul> <li>N\u00ba de contractes amb D101(04) co\u0140lectiu en pas 02 d'acceptaci\u00f3, sense M102 obertes i acceptades i que no tinguin autoconsum actiu en el contracte</li> </ul> <p>Fem l'extracci\u00f3 de dades directament sobre la db de l'erp sp2.</p> <p>Decidim fer els passos seg\u00fcents:</p>"},{"location":"models_meta/2023-03-22-dades_auto_generalitat/#1-sw_d101-llista-de-tots-els-casos-de-switching-d101-amb-motiu-de-canvi-04-i-collectiu-true","title":"1. sw_d101 = llista de tots els casos de switching d101 amb motiu de canvi 04 i collectiu true","text":"<p>S\u00f3n els casos notificats per les distris conforme aquell cups t\u00e9 una instal.laci\u00f3 d'auto colectiva</p>"},{"location":"models_meta/2023-03-22-dades_auto_generalitat/#2-sw_d102_acceptats-llista-de-casos-de-switching-d102-acceptats-rebuig-false","title":"2. sw_d102_acceptats = llista de casos de switching d102 acceptats (rebuig = false)","text":"<p><code>d1_02_accepted</code> : agafa tots els d1_02 que no han estat rebutjats</p> <pre><code>with d1_02_accepted as (\n    select d102.*, ss.id, ss.sw_id\n    from giscedata_switching_d1_02 as d102\n    left join giscedata_switching_step_header as ss on ss.id = d102.header_id\n    where rebuig = false\n)\nselect *\nfrom giscedata_switching_d1_01 as d101\nleft join giscedata_switching_step_header as ss on ss.id = d101.header_id\ninner join d1_02_accepted as d102a on ss.sw_id = d102a.sw_id\nwhere motiu_canvi = '04' and collectiu is True\n</code></pre> <p>Nosaltres comuniquem a la distri que la persona usuaria esta conforme amb les dades de l'instal.laci\u00f3. La llista que ens queda son totes les peticions d'autoconsum collectiu de les que hem iniciat el tr\u00e0mit.</p>"},{"location":"models_meta/2023-03-22-dades_auto_generalitat/#3-pol_amb-cerca-de-totes-les-polisses-amb-auto-collectiu-sctiu","title":"3. pol_amb = cerca de totes les polisses amb auto collectiu sctiu","text":"<p>Ens servir\u00e0 per a treure les polisses que la distri no s'ha esperat a tenir l'm1 al punt seg\u00fcent</p> <p><code>pol_amb</code></p> <pre><code>    select *, ps.name as cups_name, ps.id as ps_id\n    from giscedata_polissa as p\n    left join giscedata_cups_ps as ps on ps.id = p.cups\n    where autoconsumo ilike '43' or autoconsumo ilike '42'\n</code></pre>"},{"location":"models_meta/2023-03-22-dades_auto_generalitat/#4-pol_sense-sw_d102_acceptats-pol_amb-polisses-que-han-intentat-autoconsum-collectiu-i-no-ho-han-aconseguit-encara","title":"4. pol_sense = sw_d102_acceptats - pol_amb polisses que han intentat autoconsum collectiu i no ho han aconseguit (encara)","text":"<p><code>m101_col</code></p> <pre><code>    select *\n    from giscedata_switching_M1_01\n    where tipus_autoconsum = '42' or tipus_autoconsum = '43'\n</code></pre>"},{"location":"models_meta/2023-03-22-dades_auto_generalitat/#5-m1_pol_sense_a-totes-les-m1-relacionades-amb-canvi-a-42-o-43-de-les-polisses-sense-auto","title":"5. m1_pol_sense_a = totes les m1 relacionades amb canvi a 42 o 43, de les polisses sense auto","text":"<ul> <li><code>d102_accepted</code> : agafa tots els d1_02 que no han estat rebutjats, que \u00e9s el qu\u00e8 sap si \u00e9s autoconsum</li> <li><code>d102_ac_accepted</code> : tots els d102 que son d'auto col.lectiu</li> <li><code>pol_amb</code> : totes les polisses que tenen actiu autonconsum 42 o 43</li> <li><code>pol_sense_a</code> : totes els d102 que no tenen polissa amb auto co\u0140lectiu actiu</li> <li> <p><code>m101_col</code>: m101 amb autoconsum co\u0140lectiu</p> </li> <li> <p><code>query principal</code> : Tots els d102 amb autoconsum acceptat sense auto actiu a la polissa i que no tenen una M101</p> </li> </ul> <p>En <code>pol_sense_a</code> i a la <code>query principal</code> fan servir la construcci\u00f3 sql <code>left outer join</code>, que es fa amb un where <code>right.key is null</code></p> <p></p> <p>fent els joins a switching basant-se en els m1:</p> <pre><code>with d102_accepted as (\n        select ss.sw_id\n        from giscedata_switching_d1_02 as d102\n        left join giscedata_switching_step_header as ss on ss.id = d102.header_id\n        where rebuig = false\n    ),\n    d102_ac_accepted as (\n        select ps.name as cups_d1, ps.id as ps_id, distri.name as distri\n        from giscedata_switching_d1_01 as d101\n        left join giscedata_switching_step_header as ss on ss.id = d101.header_id\n        left join giscedata_switching as sw on sw.id = ss.sw_id\n        left join giscedata_cups_ps ps on ps.id = sw.cups_id\n        inner join d102_accepted as d102a on ss.sw_id = d102a.sw_id\n        left join giscedata_polissa as pol on ps.id = pol.cups\n        left join res_partner as distri on distri.id = ps.distribuidora_id\n        where motiu_canvi = '04' and collectiu is True and pol.state = 'activa'\n    ),\n    pol_amb as (\n        select ps.name as cups_name\n        from giscedata_polissa as p\n        left join giscedata_cups_ps as ps on ps.id = p.cups\n        where (autoconsumo ilike '42' or autoconsumo ilike '43')\n    ),\n    pol_sense_a as (\n        select sw.ps_id, cups_d1, distri\n        from d102_ac_accepted as sw\n        left join pol_amb as p on sw.cups_d1 = p.cups_name\n        where p.cups_name is null\n    ),\n    m101_col as (\n        select sw.cups_id, m1.id as m1_id, header_id, ss.id as ss_id, ss.sw_id\n        from giscedata_switching_M1_01 as m1\n        left join giscedata_switching_step_header as ss on ss.id = m1.header_id\n        left join giscedata_switching as sw on sw.id = ss.sw_id\n        where tipus_autoconsum = '42' or tipus_autoconsum = '43'\n    )\n    select cups_d1, ps_id, sw_id, ss_id\n    from pol_sense_a as sa\n    left join m101_col on m101_col.cups_id = sa.ps_id\n    where m101_col.m1_id is null\norder by cups_d1, sw_id, ss_id, ps_id desc\n</code></pre>"},{"location":"models_meta/2023-03-22-dades_auto_generalitat/#6-m2_pol_sense-totes-les-m2-relacionades-amb-canvi-a-42-o-43-de-les-pol_sense","title":"6. m2_pol_sense = totes les m2 relacionades amb canvi a 42 o 43, de les pol_sense","text":"<p>Reutilitzant pol_sense_a definit al pas anterior</p> <p><code>m102_col</code> : seleccionem les m102 que tenen un m101 de tipus autoconsum associat</p> <pre><code>with m101_col as (\n    select *\n    from giscedata_switching_M1_01 as m101\n    left join giscedata_switching_step_header as ss on ss.id = m101.header_id\n    where tipus_autoconsum = '42' or tipus_autoconsum = '43'\n),\nm102_col as (\n    select sw.cups_id\n    from giscedata_switching_M1_02 as m102\n    left join giscedata_switching_step_header as ss on ss.id = m102.header_id\n    left join giscedata_switching as sw on ss.sw_id = sw.id\n    inner join m101_col as m101 on m101.sw_id = ss.sw_id\n    where m102.rebuig = false\n)\nselect cups_d1, ps_id\nfrom m102_col\nleft join pol_sense_a as psa on psa.ps_id = m102_col.cups_id\norder by cups_id\n</code></pre> <p>si enlloc d'aix\u00f2 volem les polisses sense auto que NO tenen un m102, la query principal seria</p> <pre><code>select *\nfrom pol_sense_a as sa\nleft join giscedata_switching as sw on sw.cups_id = sa.ps_id\nleft join giscedata_switching_step_header as ss on ss.sw_id = sw.id\nleft join m102_col on m102_col.header_id = ss.id\nwhere m102_col.id is null\n</code></pre> <p>La query final de la necessitat dos \u00e9s, doncs:</p> <p>D'aquesta query en direm <code>query_a</code> m\u00e9s endavant.</p> <pre><code>{{ config(materialized='table') }}\n\n{# This analysis needs many tables of the erp which we do not copy at the moment #}\n\nwith d102_accepted as (\n        select ss.sw_id \n        from giscedata_switching_d1_02 as d102 \n        left join giscedata_switching_step_header as ss on ss.id = d102.header_id\n        where rebuig = false\n    ),\n    d102_ac_accepted as (\n        select ps.name as cups_d1, ps.id as ps_id, distri.name as distri\n        from giscedata_switching_d1_01 as d101\n        left join giscedata_switching_step_header as ss on ss.id = d101.header_id\n        left join giscedata_switching as sw on sw.id = ss.sw_id\n        left join giscedata_cups_ps ps on ps.id = sw.cups_id\n        inner join d102_accepted as d102a on ss.sw_id = d102a.sw_id\n        left join giscedata_polissa as pol on ps.id = pol.cups\n        left join res_partner as distri on distri.id = ps.distribuidora_id\n        where motiu_canvi = '04' and collectiu is True and pol.state = 'activa'\n    ),\n    pol_amb as (\n        select ps.name as cups_name\n        from giscedata_polissa as p\n        left join giscedata_cups_ps as ps on ps.id = p.cups\n        where (autoconsumo ilike '42' or autoconsumo ilike '43') \n    ),\n    pol_sense_a as (\n        select sw.ps_id, cups_d1, distri\n        from d102_ac_accepted as sw     \n        left join pol_amb as p on sw.cups_d1 = p.cups_name\n        where p.cups_name is null \n    ),\n    m101_col as (\n        select sw.cups_id, m1.id as m1_id, header_id, ss.id as ss_id, ss.sw_id\n        from giscedata_switching_M1_01 as m1\n        left join giscedata_switching_step_header as ss on ss.id = m1.header_id\n        left join giscedata_switching as sw on sw.id = ss.sw_id\n        where tipus_autoconsum = '42' or tipus_autoconsum = '43'\n    ),\n    m102_col as (\n        select sw.cups_id\n        from giscedata_switching_M1_02 as m102\n        left join giscedata_switching_step_header as ss on ss.id = m102.header_id\n        left join giscedata_switching as sw on ss.sw_id = sw.id\n        inner join m101_col as m101 on m101.sw_id = ss.sw_id\n        where m102.rebuig = false\n    )            \n    select cups_d1, distri\n    from m102_col \n    inner join pol_sense_a as sa on m102_col.cups_id = sa.ps_id    \n    group by cups_d1, distri\n</code></pre> <p>Aquesta query parteix dels d1 per a arribar als casos M. Una manera alternativa \u00e9s partir dels casos de switching i fer servir l'additional_info.</p>"},{"location":"models_meta/2023-03-22-dades_auto_generalitat/#via-el-cas-de-switching","title":"via el cas de switching","text":"<p>D'aquesta query en direm <code>query_b</code></p> <pre><code>{{ config(materialized='table') }}\n\n{# This analysis needs many tables of the erp which we do not copy at the moment #}\n\nselect cups_input as cups_qb, distri.name as distri\n        from giscedata_switching as sw\n        left join giscedata_polissa as pol on pol.id = sw.cups_polissa_id\n        left join res_partner as distri on distri.id = pol.distribuidora\n        where (sw.additional_info like '%-&gt; 42;%' or sw.additional_info like '%-&gt; 43;%')\n        and sw.finalitzat is null\n        and pol.state = 'activa'\n        and sw.proces_id = 3\n        and sw.step_id = 22\n        and pol.autoconsumo != '43' and pol.autoconsumo != '42'\n        group by cups_input, distri.name\n</code></pre> <p>Poden haver-hi difer\u00e8ncies entre les dues queries perqu\u00e8 una</p> <ul> <li>id_polissa = 312905: aquest surt amb la query_b pero no amb la query_a perque no t\u00e9 D1</li> <li>id_polissa = 93247: aquest surt amb la query_a pero no la query_b perque tot i tenir una 02 aceptada, tb te una 05.</li> <li>id_polissa = 146266: idem, hi ha un 02 acceptaci\u00f3 pero hi ha tb un 04 (lo capta la query_a no la query_b)</li> </ul>"},{"location":"models_meta/2023-03-22-dades_auto_generalitat/#queries-que-ens-han-estat-utils-en-lanalisi","title":"Queries que ens han estat \u00fatils en l'an\u00e0lisi","text":"<p>La query estat_del_cas_de_giscedata_switching agafa 6 processos de switching d'auto.</p> <p>Tamb\u00e9 ha estat \u00fatil saber que giscedata_switching t\u00e9 informaci\u00f3 tant del tipus de cas ATR, es a dir, M, C, D, etc (columna proces_id = giscedata_switching_proces) com del pas en el que es troba el cas ATR (columna step_id = giscedata_switching_step)</p>"},{"location":"models_meta/2023-03-22-dades_auto_generalitat/#necessitat-2","title":"Necessitat 2","text":"<ul> <li>N\u00ba de contractes amb una M cap a auto 42 i 43, en estat tancat, i que en el contracte no aparegui autoconsum actiu.</li> </ul> <p>via erpclient:</p> <p>cups_iniciats = cerquem tots els cups que en algun moment han fet un tramit d'auto co\u0140lectiu:</p> <pre><code>tots els casos\n&gt; 'Informaci\u00f3 adicional': '-&gt; 42;' i 'Proces': M i 'estat': 'tancat'\n&gt; 'Informaci\u00f3 adicional': '-&gt; 43;' i 'Proces': M i 'estat': 'tancat'\n</code></pre> <p>cups_auto_col = cercar a totes les polisses: auto 42 i a totes les polisses: auto 43  cups_sense = cups_iniciats - cups_auto_col</p> <p>fent-ho a trav\u00e9s de la base de dades, quedaria aix\u00ed</p> <pre><code>{{ config(materialized='table') }}\n\n{# This analysis needs many tables of the erp which we do not copy at the moment #}\n\nwith d102_accepted as (\n        select ss.sw_id\n        from giscedata_switching_d1_02 as d102\n        left join giscedata_switching_step_header as ss on ss.id = d102.header_id\n        where rebuig = false\n    ),\n    d102_ac_accepted as (\n        select ps.name as cups_d1, ps.id as ps_id, distri.name as distri\n        from giscedata_switching_d1_01 as d101\n        left join giscedata_switching_step_header as ss on ss.id = d101.header_id\n        left join giscedata_switching as sw on sw.id = ss.sw_id\n        left join giscedata_cups_ps ps on ps.id = sw.cups_id\n        inner join d102_accepted as d102a on ss.sw_id = d102a.sw_id\n        left join giscedata_polissa as pol on ps.id = pol.cups\n        left join res_partner as distri on distri.id = ps.distribuidora_id\n        where motiu_canvi = '04' and collectiu is True and pol.state = 'activa'\n    ),\n    pol_amb as (\n        select ps.name as cups_name\n        from giscedata_polissa as p\n        left join giscedata_cups_ps as ps on ps.id = p.cups\n        where (autoconsumo ilike '42' or autoconsumo ilike '43')\n    ),\n    pol_sense_a as (\n        select sw.ps_id, cups_d1, distri\n        from d102_ac_accepted as sw\n        left join pol_amb as p on sw.cups_d1 = p.cups_name\n        where p.cups_name is null\n    ),\n    m101_col as (\n        select sw.cups_id, m1.id as m1_id, header_id, ss.id as ss_id, ss.sw_id\n        from giscedata_switching_M1_01 as m1\n        left join giscedata_switching_step_header as ss on ss.id = m1.header_id\n        left join giscedata_switching as sw on sw.id = ss.sw_id\n        where tipus_autoconsum = '42' or tipus_autoconsum = '43'\n    ),\n    m102_col as (\n        select sw.cups_id\n        from giscedata_switching_M1_02 as m102\n        left join giscedata_switching_step_header as ss on ss.id = m102.header_id\n        left join giscedata_switching as sw on ss.sw_id = sw.id\n        inner join m101_col as m101 on m101.sw_id = ss.sw_id\n        where m102.rebuig = false\n    ),\n    pol_tramit_actiu as (\n        select cups_d1, distri\n        from m102_col\n        inner join pol_sense_a as sa on m102_col.cups_id = sa.ps_id\n        group by cups_d1, distri\n    ),\n    pol_1M_tancada as (\n        select cups_input as cups_qb, distri.name as distri\n        from giscedata_switching as sw\n        left join giscedata_polissa as pol on pol.id = sw.cups_polissa_id\n        left join res_partner as distri on distri.id = pol.distribuidora\n        where (sw.additional_info like '%-&gt; 42;%' or sw.additional_info like '%-&gt; 43;%')\n        and sw.finalitzat is not null\n        and pol.state = 'activa'\n        and sw.proces_id = 3\n        and pol.autoconsumo != '43' and pol.autoconsumo != '42'\n        group by cups_input, distri.name\n    )\n    select mt.cups_qb as cups, mt.distri\n    from pol_1m_tancada as mt\n    left join pol_tramit_actiu as ta on mt.cups_qb = ta.cups_d1\n    where ta.cups_d1 is null\n</code></pre> <p>Per la necessitat 1, m\u00e9s tard hem descobert que podiem fer servir la mateixa t\u00e8cnica per\u00f2 seleccionant els que si que havien finalitzat (<code>query_b</code>). \u00c9s una manera m\u00e9s senzilla i podeu saltar-vos tot el proc\u00e9s que hem seguit per fer l'an\u00e0lisi via d1s. Ho trobareu al final de tot d'aquest document.</p>"}]}